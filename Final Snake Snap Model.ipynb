{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce635500-327b-4f32-a484-66b764f4c36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy==1.26.4 tensorflow opencv-python scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e578154-3cfd-4b1c-882a-26c297a72aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed2f107-32b6-41fb-8e38-2f3a39904ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class SnakeBreedDetector:\n",
    "    def __init__(self, input_shape=(224, 224, 3), data_dir=None, batch_size=32):\n",
    "        self.input_shape = input_shape\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.class_names = self._get_class_names() if data_dir else None\n",
    "        self.num_classes = len(self.class_names) if self.class_names else 0\n",
    "        self.model = self._build_model()\n",
    "        \n",
    "    def _get_class_names(self):\n",
    "        \"\"\"Extract class names from subfolder names inside data_dir.\"\"\"\n",
    "        # Sort to keep order consistent.\n",
    "        return sorted(os.listdir(self.data_dir))\n",
    "    \n",
    "    def _build_model(self):\n",
    "        model = models.Sequential([\n",
    "            # Convolutional layers with BatchNormalization for faster convergence.\n",
    "            layers.Conv2D(32, (3, 3), activation='relu', input_shape=self.input_shape),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            \n",
    "            layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            \n",
    "            layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            \n",
    "            layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            \n",
    "            layers.Flatten(),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(512, activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dense(self.num_classes, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    def preprocess_image(self, image_path):\n",
    "        img = cv2.imread(image_path)\n",
    "        img = cv2.resize(img, (self.input_shape[0], self.input_shape[1]))\n",
    "        img = img / 255.0  # Normalize pixel values.\n",
    "        return np.expand_dims(img, axis=0)\n",
    "    \n",
    "    def train(self, epochs=50):\n",
    "        # Use ImageDataGenerator with data augmentation and a validation split.\n",
    "        datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            validation_split=0.2  # Reserve 20% of data for validation.\n",
    "        )\n",
    "        \n",
    "        train_generator = datagen.flow_from_directory(\n",
    "            self.data_dir,\n",
    "            target_size=(self.input_shape[0], self.input_shape[1]),\n",
    "            batch_size=self.batch_size,\n",
    "            class_mode='categorical',\n",
    "            subset='training'\n",
    "        )\n",
    "        \n",
    "        validation_generator = datagen.flow_from_directory(\n",
    "            self.data_dir,\n",
    "            target_size=(self.input_shape[0], self.input_shape[1]),\n",
    "            batch_size=self.batch_size,\n",
    "            class_mode='categorical',\n",
    "            subset='validation'\n",
    "        )\n",
    "        \n",
    "        # Callbacks for advanced training:\n",
    "        early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        checkpoint = callbacks.ModelCheckpoint('best_snake_detector.h5', monitor='val_loss', save_best_only=True)\n",
    "        reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
    "        \n",
    "        history = self.model.fit(\n",
    "            train_generator,\n",
    "            epochs=epochs,\n",
    "            validation_data=validation_generator,\n",
    "            callbacks=[early_stop, checkpoint, reduce_lr]\n",
    "        )\n",
    "        return history\n",
    "    \n",
    "    def predict(self, image_path):\n",
    "        img = self.preprocess_image(image_path)\n",
    "        predictions = self.model.predict(img)[0]\n",
    "        predicted_class = np.argmax(predictions)\n",
    "        predicted_species = self.class_names[predicted_class]\n",
    "        confidence = np.max(predictions)\n",
    "        return predicted_species, confidence\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        self.model.save(path)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_model(cls, path, class_names):\n",
    "        detector = cls()\n",
    "        detector.model = tf.keras.models.load_model(path)\n",
    "        detector.class_names = class_names\n",
    "        detector.num_classes = len(class_names)\n",
    "        return detector\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Provide the path to your dataset containing subfolders for each snake species.\n",
    "    dataset_path = \"C:\\\\Users\\\\RENTKAR\\\\Downloads\\\\Telegram Desktop\\\\snake_data\"\n",
    "    \n",
    "    detector = SnakeBreedDetector(data_dir=dataset_path, batch_size=32)\n",
    "    \n",
    "    # Train the model (with early stopping, checkpointing, and learning rate reduction)\n",
    "    detector.train(epochs=50)\n",
    "    \n",
    "    # Save the trained model.\n",
    "    detector.save_model(\"snake_detector.h5\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afaa869-50f3-4912-8aae-8875a94ee266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model for inference\n",
    "detector = SnakeBreedDetector.load_model(\"snake_detector.h5\", class_names=detector.class_names)\n",
    "\n",
    "# Make a prediction\n",
    "species, confidence = detector.predict(\"C:\\\\Users\\\\RENTKAR\\\\Pictures\\\\ScreenShots\\\\python sample testing.png\")\n",
    "\n",
    "print(f\"Predicted species: {species}\")\n",
    "print(f\"Confidence: {confidence:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
